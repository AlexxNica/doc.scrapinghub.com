==========
Extensions
==========
You can easily extend the capabilities of your crawlers running on Scrapy Cloud by using Scrapy extensions. Most extensions are available as Python packages on PyPI, so setting them up is just a matter of pip installing them in your environment and including them as a requirement in your project's requirements file. Let's dive in some useful extensions.


Scrapy Magic Fields
===================
`Scrapy-magicfields <https://github.com/scrapy-plugins/scrapy-magicfields>`_ is a plugin that you can use to generate and add fields to scraped items. For example, you may want to include something like this in each spider’s extraction logic::

    item['timestamp'] = datetime.now()
    item['url'] = response.url

It might be quick and simple to do if you have only a few spiders, but it can become a hassle if your project is larger.

The scrapy-magicfields plugin takes care of this for you. It adds extra-fields to all items scraped in a crawl, based on configurations you define in your project's settings.py.

For instance, we could define an extra field called `log` with the following pattern::

    'Scraped from $response:url at $time'

For each item generated by the spiders, the plugin would include a field called 'log' having something like this as its value::

    'Scraped from books.toscrape.com at 2016-09-08 17:25:09'

As you can see, ``$response`` and ``$time`` are magic variables that are populated by the plugin. There's a list of supported magic variables in the `plugin project at Github <https://github.com/scrapy-plugins/scrapy-magicfields#supported-magic-variables>`_.

How to use it?
--------------
First, install ``scrapy-magicfields`` in your environment using pip::

    $ pip install scrapy-magicfields

Then, add ``MagicFieldsMiddleware`` to ``SPIDER_MIDDLEWARES`` in your project settings.py::

    SPIDER_MIDDLEWARES = {
        'scrapy_magicfields.MagicFieldsMiddleware': 100,
    }

In the same file, define the fields you want to include in the scraped items using the magic variables provided by the plugin. For example::

    MAGIC_FIELDS = {
        "timestamp": "$time",
        "spider": "$spider:name",
        "url": "scraped from $response:url",
    }

From now on, every item generated by your spider will include additional information such as::

    {
        …
        "timestamp": "2016-07-15 17:25:09",
        "spider": "toscrape",
        "url": "scraped from http://books.toscrape.com",
        …
    }

You can also use regular expressions to filter the data that you want to extract. For example, you might want to get only the domain name from the scraped URL and store it in a separate field::

    MAGIC_FIELDS = {
        "timestamp": "$time",
        "spider": "$spider:name",
        "url": "scraped from $response:url",
        "domain": "$response:url,r'https?://([\w\.]+)/']",
    }

Check out further information on: http://github.com/scrapy-plugins/scrapy-magicfields

How to deploy it to Scrapy Cloud
--------------------------------
To use this plugin in Scrapy Cloud, you just have to add the following line to 
your project's `requirements.txt`::

    scrapy-magicfields

And make sure the ``requirements.txt`` is referenced from your `scrapinghub.yml`::

    requirements_file: path/to/your/requirements.txt

And then deploy your project as you always do with ``shub deploy``.
