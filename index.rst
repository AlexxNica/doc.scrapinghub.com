Scrapinghub Documentation
=========================

Scrapinghub is the most advanced platform for deploying and running web crawlers (also known as *spiders* or *scrapers*). It allows you to build crawlers easily, deploy them instantly and scale them on demand, without having to manage servers, backups or cron jobs. Everything is stored in a highly available database and retrievable using an API.

Spiders can be written in Python using `Scrapy`_, built visually using `Portia`_ tool or both. Spiders are grouped into projects. Each spider run is known as a *job*.

Here you will find reference documentation. For more articles, guides and other help resources please visit our `Knowledge Base`_ in our `Support center`_.

Products
--------

* :doc:`scrapy-cloud`
* :doc:`crawlera`
* :doc:`portia`

Platform Concepts, Features & Tools
-----------------------------------

* :doc:`dash`
* :doc:`shub`
* :doc:`addons`

.. _api:

API
---

.. toctree::
   :maxdepth: 2

* :ref:`api-overview`
* :ref:`api-comments`
* :ref:`api-eggs`
* :ref:`api-jobs`
* :ref:`api-activity`
* :ref:`api-collections`
* :ref:`api-frontier`
* :ref:`api-items`
* :ref:`api-logs`
* :ref:`api-requests`

.. toctree::
   :hidden:

   api/overview

   scrapy-cloud
   crawlera
   portia

   dash
   shub
   addons

.. _Support center: https://support.scrapinghub.com
.. _Knowledge Base: https://support.scrapinghub.com/forum/24895-knowledge-base/
.. _Scrapy: http://scrapy.org
.. _Portia: https://www.scrapinghub.com/portia/
