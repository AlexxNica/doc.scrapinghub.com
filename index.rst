Scrapinghub Documentation
=========================

Scrapinghub is the most advanced platform for deploying and running web crawlers (also known as *spiders* or *scrapers*). It allows you to build crawlers easily, deploy them instantly and scale them on demand, without having to manage servers, backups or cron jobs. Everything is stored in a highly available database and retrievable using an API.

Spiders can be written in `Scrapy`_ or built using the `Autoscraping`_ tool, and they are grouped into projects. Each spider run is known as a *job*.

Here you will find reference documentation. For articles, guides and other help resouces please visit our `Support site`_ and look for the *Articles* section of
each product.

Table of Contents
-----------------

.. toctree::
   :maxdepth: 2

   pricing
   dash
   scrapy-cloud
   api
   autoscraping
   addons

.. _Support site: http://support.scrapinghub.com
.. _Scrapy: http://scrapy.org
.. _Autoscraping: http://scrapinghub.com/autoscraping
